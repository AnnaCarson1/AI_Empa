{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Problem 1: Confirmation of competition contents\n",
    "What to learn and what to predict?\n",
    "The goal is to predict the probability of a customer defaulting on a loan. The dataset includes various features related to the client's background, financial history, and other relevant information.\n",
    "\n",
    "What kind of file to create and submit to Kaggle?\n",
    "You need to submit a CSV file with two columns: SK_ID_CURR (the unique identifier for each loan application) and TARGET (the predicted probability of default).\n",
    "\n",
    "What kind of index value will be used to evaluate the submissions?\n",
    "The submissions will be evaluated based on the area under the ROC curve (AUC-ROC). The AUC-ROC score measures the model's ability to distinguish between the positive and negative classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: Creating a baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/jolie/anaconda3/lib/python3.9/site-packages (1.3.1)\n",
      "Requirement already satisfied: pandas in /home/jolie/anaconda3/lib/python3.9/site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/jolie/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/jolie/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/jolie/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jolie/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/jolie/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jolie/anaconda3/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jolie/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
      "dtypes: float64(65), int64(41), object(16)\n",
      "memory usage: 286.2+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jolie/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 0.7453642994738289\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"application_train.csv\")\n",
    "\n",
    "# Basic analysis\n",
    "print(train_data.shape)\n",
    "print(train_data.info())\n",
    "\n",
    "# Select features and target variable\n",
    "X = train_data.drop(['TARGET'], axis=1)\n",
    "y = train_data['TARGET']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Combine preprocessing with the logistic regression model\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', LogisticRegression())])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate using AUC-ROC\n",
    "auc_roc = roc_auc_score(y_val, y_pred)\n",
    "print(f\"AUC-ROC Score: {auc_roc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score on Validation Data: 0.7091623165404071\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"application_train.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = train_data.drop(['TARGET'], axis=1)\n",
    "y = train_data['TARGET']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Combine preprocessing with the random forest classifier\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate using AUC-ROC\n",
    "auc_roc = roc_auc_score(y_val, y_pred)\n",
    "print(f\"AUC-ROC Score on Validation Data: {auc_roc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_data = pd.read_csv(\"./application_test.csv\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict_proba(test_data)[:, 1]\n",
    "\n",
    "# Create a submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'SK_ID_CURR': test_data['SK_ID_CURR'],\n",
    "    'TARGET': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv(\"credit_default_submission.csv\", index=False)\n",
    "\n",
    "# Display a message indicating successful submission\n",
    "print(\"Submission file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score on Validation Data (Feature Engineering): 0.7080162649654906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"application_train.csv\")\n",
    "\n",
    "# Feature Engineering - Pattern 1: Adding interaction terms\n",
    "train_data['EXT_SOURCE_3 * DAYS_BIRTH'] = train_data['EXT_SOURCE_3'] * train_data['DAYS_BIRTH']\n",
    "\n",
    "# Feature Engineering - Pattern 2: Creating a debt-to-income ratio feature\n",
    "train_data['debt_to_income_ratio'] = train_data['AMT_CREDIT'] / train_data['AMT_INCOME_TOTAL']\n",
    "\n",
    "# Feature Engineering - Pattern 3: Handling family size outliers\n",
    "train_data['CNT_FAM_MEMBERS'] = train_data['CNT_FAM_MEMBERS'].clip(1, 5)\n",
    "\n",
    "# Feature Engineering - Pattern 4: Creating a binary flag for missing values in EXT_SOURCE_3\n",
    "train_data['EXT_SOURCE_3_missing'] = train_data['EXT_SOURCE_3'].isnull()\n",
    "\n",
    "# Feature Engineering - Pattern 5: Log-transforming skewed features\n",
    "skewed_features = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY']\n",
    "for feature in skewed_features:\n",
    "    train_data[feature + '_log'] = np.log1p(train_data[feature])\n",
    "\n",
    "# Select features and target variable\n",
    "X = train_data.drop(['TARGET'], axis=1)\n",
    "y = train_data['TARGET']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Combine preprocessing with the random forest classifier\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate using AUC-ROC\n",
    "auc_roc = roc_auc_score(y_val, y_pred)\n",
    "print(f\"AUC-ROC Score on Validation Data (Feature Engineering): {auc_roc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
